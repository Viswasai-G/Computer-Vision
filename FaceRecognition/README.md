# Face Recognition using Principal Component Analysis and HOG feature


The database used in this project is obtained from AT&T Laboratories, Cambridge. The database contains 40 subjects with each subject having 10 images. The images have variance in lighting, expressions and facial details such as glasses etc. The images were taken at the AT&T lab between 1992 and 1994. All images are of 112x92 size with 256 levels of gray per pixel. The database can be obtained from http://www.cl.cam.ac.uk/Research/DTG/attarchive:pub/data/att_faces.tar.Z.

# Principal Component Analysis

Eigenfaces is the name given to a set of eigenvectors when they are used in computer vision to classify human faces in the given database. A set of the eigenfaces are generated by performing Principal Component Analysis (PCA). The overview of this method is as follows:
	First, we take the database and convert each image of the face into a vector of N dimension. Say we have an image of the face of a person with the dimensions 20x20, then we convert this image into a 400x1 vector. We perform this for all images in the given database. Then, a normalized face vector or average face vector all obtained face vectors is derived. Since we have a normalized value representing the entire faces of the database, we subtract the value of each face vector present in the database to get only the variance of each face with the given average face vector. We then generate a covariance matrix that represent the eigenvectors of all the face vectors. These eigenvectors are also known as eigen faces and each eigen face represents an individual in the database. The eigenvector matrix that is formed is of very large size and can be an overhead for computation. For example, we have 10 images in the database, then we get the overall covariance matrix to be of size 400x400 since each vector is of size 400. As the size of each image and the number of images in the database increase, the size of the covariance matrix has exponential increase with respect to the size of the image. Therefore, to reduce the size of the covariance matrix, we compute a covariance matrix by dimensionality reduction. By doing a dimensionality reduction, we can obtain a covariance matrix of the given database as a size of 10x10 as opposed to a 400x400 matrix. This dimensionality reduction increases linearly with the increase in number of images in the database. Then, the given input image face (test image) is converted into an eigen vector and compared against the covariance matrix containing all the faces. The value that has least normalized Euclidean value is selected and displayed as the matched face.


#  Histogram of Gradients

Histogram of Oriented Gradients is a feature descriptor used in computer vision and image processing for the purpose of object detection. It generalizes the count of occurrences of gradient in an orientation in localized portions of the image. We use this feature descriptor to count the occurrences of gradients for each face in the image. The working of this system is as follows:
First, we split the given database of facial images into 70% training images and 30% test images. Since the database used in the experiment consists of 10 images per individual, the training data for each individual is 7 images and the remaining 3 are test images used to validate the accuracy of the system. We use the training images and extract the Histogram of Oriented Gradients descriptor from each image. These extracted descriptors are then supplied to a multi-class classifier know as Error Correcting Output Codes (ECOC). This model uses multiple binary classifiers to classify each face in the given database using the given descriptors. The model is trained on the features using 7 correct instances and 7 wrong instances for each class type. The model is then validated using the test data to determine the accuracy of this system. 
